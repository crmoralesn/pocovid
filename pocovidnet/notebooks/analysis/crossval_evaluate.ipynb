{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from imutils import paths\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score,matthews_corrcoef\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\"covid\", \"pneumonia\", \"regular\", \"uninformative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pocovidnet.evaluate_covid19 import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(preds, gt, vid_filenames):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    \tpreds: predicted classes (1-d list of class_names or integers)\n",
    "        gt: list of same size with ground truth labels\n",
    "        vid_filenames: list of filenames\n",
    "    \"\"\"\n",
    "    preds = np.asarray(preds)\n",
    "    gt = np.asarray(gt)\n",
    "    vids = np.asarray([vid.split(\".\")[0] for vid in vid_filenames])\n",
    "    vid_preds_out = []\n",
    "    for v in np.unique(vids):\n",
    "        preds_video = preds[vids==v]\n",
    "        gt_check = np.unique(gt[vids==v])\n",
    "        assert len(gt_check)==1, \"gt must have the same label for the whole video\"\n",
    "        labs, pred_counts = np.unique(preds_video, return_counts=True)\n",
    "        # take label that is predicted most often\n",
    "        vid_pred = labs[np.argmax(pred_counts)]\n",
    "        # print(\"preds for video:\", preds_video)\n",
    "        # print(v[:3], vid_pred, gt_check[0])\n",
    "        vid_preds_out.append([v, vid_pred, gt_check[0]])\n",
    "    # print(\"video accuracy (majority):\", accuracy_score([p[1] for p in vid_preds_out], [p[2] for p in vid_preds_out]))\n",
    "    return vid_preds_out\n",
    "        \n",
    "def average_certainty(preds_logits, gt, vid_filenames):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    \tpreds: predicted classes (1-d list of class_names or integers)\n",
    "        gt: list of same size with ground truth labels\n",
    "        vid_filenames: list of filenames\n",
    "    \"\"\"\n",
    "    preds_logits = np.asarray(preds_logits)\n",
    "    gt = np.asarray(gt)\n",
    "    vid_preds_out = []\n",
    "    vids = np.array([vid.split(\".\")[0] for vid in vid_filenames])\n",
    "    for v in np.unique(vids):\n",
    "        preds_video_logits = preds_logits[vids==v]\n",
    "        preds_video = np.sum(preds_video_logits, axis=0)\n",
    "        # print(\"preds for video:\", preds_video)\n",
    "        gt_check = np.unique(gt[vids==v])\n",
    "        assert len(gt_check)==1, \"gt must have the same label for the whole video\"\n",
    "        # take label that is predicted most often\n",
    "        vid_pred = np.argmax(preds_video)\n",
    "        # print(v, vid_pred, gt_check[0])\n",
    "        vid_preds_out.append([v, vid_pred, gt_check[0]])\n",
    "    # print(\"video accuracy (certainty):\", accuracy_score([p[1] for p in vid_preds_out], [p[2] for p in vid_preds_out]))\n",
    "    return vid_preds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcc_multiclass(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    mcc_out = []\n",
    "    for classe in np.unique(y_true):\n",
    "        y_true_binary = (y_true==classe).astype(int)\n",
    "        y_pred_binary = (y_pred==classe).astype(int)\n",
    "        mcc_out.append(matthews_corrcoef(y_true_binary, y_pred_binary))\n",
    "    return mcc_out\n",
    "def specificity(y_true, y_pred):\n",
    "    # true negatives / negatives\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    spec_out = []\n",
    "    for classe in np.unique(y_true):\n",
    "        negatives = np.sum((y_true!=classe).astype(int))\n",
    "        tn = np.sum((y_pred[y_true!=classe]!=classe).astype(int))\n",
    "        spec_out.append(tn/negatives)\n",
    "    return spec_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation script for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saved_logits, saved_gt, saved_files = [], [], []\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"------------- SPLIT \", i, \"-------------------\")\n",
    "    # define data input path\n",
    "    path = \"../../data/cross_validation/split\"+str(i)\n",
    "    \n",
    "    train_labels, test_labels, test_files = [], [], []\n",
    "    train_data, test_data = [], []\n",
    "\n",
    "    # loop over the image paths (train and test)\n",
    "    for imagePath in paths.list_images(path):\n",
    "\n",
    "        # extract the class label from the filename\n",
    "        label = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "        # load the image, swap color channels, and resize it to be a fixed\n",
    "        # 224x224 pixels while ignoring aspect ratio\n",
    "        image = cv2.imread(imagePath)\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # image = cv2.resize(image, (224, 224))\n",
    "\n",
    "        # update the data and labels lists, respectively\n",
    "        test_labels.append(label)\n",
    "        test_data.append(image)\n",
    "        test_files.append(imagePath.split(os.path.sep)[-1])\n",
    "\n",
    "    # build ground truth data\n",
    "    gt_class_idx = np.array([CLASSES.index(lab) for lab in test_labels])\n",
    "    model = None\n",
    "    # load model\n",
    "    model = Evaluator(weights_dir=\"NasNet_F\", ensemble=False, split=i, num_classes=len(CLASSES), model_id=\"nasnet\")\n",
    "    print(\"testing on n_files:\", len(test_data))\n",
    "    \n",
    "    # MAIN STEP: feed through model and compute logits\n",
    "    logits = np.array([model(img) for img in test_data])\n",
    "    \n",
    "    # remember for evaluation:\n",
    "    saved_logits.append(logits)\n",
    "    saved_gt.append(gt_class_idx)\n",
    "    saved_files.append(test_files)\n",
    "    \n",
    "    # output the information\n",
    "    predIdxs = np.argmax(logits, axis=1)\n",
    "    \n",
    "    print(\n",
    "    classification_report(\n",
    "        gt_class_idx, predIdxs, target_names=CLASSES\n",
    "        )\n",
    "    )\n",
    "\n",
    "    vid_preds_certainty = average_certainty(logits, gt_class_idx, np.array(test_files))\n",
    "    vid_preds_majority = majority_vote(predIdxs, gt_class_idx, np.array(test_files))\n",
    "    print(\"video accuracies:\", vid_preds_certainty, vid_preds_majority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"NASF_fold4.dat\", \"wb\") as outfile:\n",
    "    pickle.dump((logits, gt_class_idx, test_files), outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"model_comparison/results_segment.dat\", \"wb\") as outfile:\n",
    "    pickle.dump((saved_logits, saved_gt, saved_files), outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect single folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_logits, saved_gt, saved_files = [], [], []\n",
    "for i in range(5):\n",
    "    with open(\"NASF_fold\"+str(i)+\".dat\", \"rb\") as outfile:\n",
    "        (logits, gt, files) = pickle.load(outfile)\n",
    "        saved_logits.append(logits)\n",
    "        saved_gt.append(gt)\n",
    "        saved_files.append(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform from uninformative class ones to general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_logits, new_gt, new_files = [], [], []\n",
    "counter = 0\n",
    "for i in range(5):\n",
    "    gt_inds = np.where(np.array(saved_gt[i])<3)[0]\n",
    "    counter += len(gt_inds)\n",
    "    new_logits.append(np.array(saved_logits[i])[gt_inds, :3])\n",
    "    new_gt.append(np.array(saved_gt[i])[gt_inds])\n",
    "    new_files.append(np.array(saved_files[i])[gt_inds])\n",
    "    \n",
    "import pickle\n",
    "with open(\"../encoding_3.dat\", \"wb\") as outfile:\n",
    "    pickle.dump((new_logits, new_gt, new_files), outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load outputs (takes the dat files that was saved from the evaluation above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle # \n",
    "with open(\"../encoding_3.dat\", \"rb\") as outfile:\n",
    "    (saved_logits, saved_gt, saved_files) = pickle.load(outfile)\n",
    "    \n",
    "CLASSES = [\"covid\", \"pneumonia\", \"regular\"] # , \"uninformative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute scores of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the reports and accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reports = []\n",
    "accs = []\n",
    "bal_accs = []\n",
    "# vid_accs, _, vid_accs_bal, _ = video_accuracy(saved_logits, saved_gt, saved_files)\n",
    "for s in range(5):\n",
    "    gt_s = saved_gt[s]\n",
    "    pred_idx_s = np.argmax(np.array(saved_logits[s]), axis=1)\n",
    "    report = classification_report(\n",
    "        gt_s, pred_idx_s, target_names=CLASSES, output_dict=True\n",
    "        )\n",
    "    mcc_scores = mcc_multiclass(gt_s, pred_idx_s)\n",
    "    spec_scores = specificity(gt_s, pred_idx_s)\n",
    "    for i, cl in enumerate(CLASSES):\n",
    "        report[cl][\"mcc\"] = mcc_scores[i]\n",
    "        report[cl][\"specificity\"] = spec_scores[i]\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df = df.drop(columns=\"support\")\n",
    "    df[\"accuracy\"] = [report[\"accuracy\"] for _ in range(len(df))]\n",
    "    bal = balanced_accuracy_score(gt_s, pred_idx_s)\n",
    "    df[\"balanced\"] = [bal for _ in range(len(df))]\n",
    "    # df[\"video\"] = vid_accs[s]\n",
    "    # df[\"video_balanced\"] = vid_accs_bal[s]\n",
    "    # print(df[:len(CLASSES)])\n",
    "    # print(np.array(df)[:3,:])\n",
    "    accs.append(report[\"accuracy\"])\n",
    "    bal_accs.append(balanced_accuracy_score(gt_s, pred_idx_s))\n",
    "    # df = np.array(report)\n",
    "    all_reports.append(np.array(df)[:len(CLASSES)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arr = np.around(np.mean(all_reports, axis=0), 2)\n",
    "df_classes = pd.DataFrame(df_arr, columns=[\"Precision\", \"Recall\", \"F1-score\", \"MCC\", \"Specificity\", \"Accuracy\", \"Balanced\"], index=CLASSES)\n",
    "df_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std = np.around(np.std(all_reports, axis=0), 2)\n",
    "df_std = pd.DataFrame(df_std, columns=[\"Precision\", \"Recall\", \"F1-score\", \"MCC\", \"Specificity\", \"Accuracy\", \"Balanced\"], index=CLASSES)\n",
    "df_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classes = df_classes[[\"Accuracy\", \"Balanced\", \"Precision\", \"Recall\",\"Specificity\", \"F1-score\", \"MCC\"]]\n",
    "df_std = df_std[[\"Accuracy\", \"Balanced\", \"Precision\", \"Recall\",\"Specificity\", \"F1-score\", \"MCC\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classes.to_csv(\"model_comparison/encoding_3_mean.csv\")\n",
    "df_std.to_csv(\"model_comparison/encoding_3_std.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Output accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The accuracy and balanced accuracy of our model are:\")\n",
    "print(np.around(accs,2),np.around(bal_accs,2))\n",
    "print(\"MEAN ACC:\", round(np.mean(accs), 2), \"MEAN BAL ACC:\", round(np.mean(bal_accs),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The accuracy and balanced accuracy of our model are:\")\n",
    "print(np.around(accs,2),np.around(bal_accs,2))\n",
    "print(\"MEAN ACC:\", round(np.mean(accs), 2), \"MEAN BAL ACC:\", round(np.mean(bal_accs),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make table of results distinguished by classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_nr_videos(saved_files):\n",
    "    file_list = []\n",
    "    for sav in saved_files:\n",
    "        file_list.extend(sav)\n",
    "    assert len(np.unique(file_list)) == len(file_list)\n",
    "    cutted_files = [f.split(\".\")[0] for f in file_list]\n",
    "    print(\"number of videos\", len(np.unique(cutted_files)))\n",
    "    vid_file_labels = [v[:3].lower() for v in np.unique(cutted_files)]\n",
    "    print(len(vid_file_labels))\n",
    "    print(np.unique(vid_file_labels, return_counts=True))\n",
    "    lab, counts = np.unique(vid_file_labels, return_counts=True)\n",
    "    return counts.tolist()\n",
    "\n",
    "def compute_specificity(all_cms):\n",
    "    \"\"\"\n",
    "    Function to compute the specificity from confusion matrices\n",
    "    all_cms: array of size 5 x 3 x 3 --> confusion matrix for each fold\n",
    "    \"\"\"\n",
    "    specificities_fold = []\n",
    "    for k in range(len(all_cms)):\n",
    "        arr = all_cms[k]\n",
    "        overall = np.sum(arr)\n",
    "        specificity = []\n",
    "        for i in range(len(arr)):\n",
    "            tn_fp = overall - np.sum(arr[i])\n",
    "            # print(bottom_six)\n",
    "            fp = 0\n",
    "            for j in range(len(arr)):\n",
    "                if i!=j:\n",
    "                    fp += arr[j, i]\n",
    "            spec = (tn_fp-fp)/tn_fp\n",
    "            # print(\"tn\", tn_fp-fp, \"tn and fp:\", tn_fp)\n",
    "            # print(spec)\n",
    "            specificity.append(spec)\n",
    "        specificities_fold.append(specificity)\n",
    "    out_spec = np.mean(np.asarray(specificities_fold), axis=0)\n",
    "    return np.around(out_spec, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum up confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cms = np.zeros((5,3,3))\n",
    "for s in range(5):\n",
    "    # print(saved_files[s])\n",
    "    gt_s = saved_gt[s]\n",
    "    pred_idx_s = np.argmax(np.array(saved_logits[s]), axis=1)\n",
    "    assert len(gt_s)==len(pred_idx_s)\n",
    "    cm = np.array(confusion_matrix(gt_s, pred_idx_s))\n",
    "    all_cms[s] = cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add specificit, number of frames etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.sum(all_cms, axis=0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classes[\"Specificity\"] = np.around(compute_specificity(all_cms),2)\n",
    "df_classes[\"Frames\"] = np.sum(np.sum(all_cms, axis=0), axis=1).astype(int).tolist()\n",
    "# df_classes[\"Videos/Images\"] = comp_nr_videos(saved_files)\n",
    "# df_classes = df_classes.drop(columns=[\"Support\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classes.to_csv(\"average_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD MODEL:\n",
    "df_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to Covid-Net\n",
    "\n",
    "Manually copied data from txt fil\n",
    "\n",
    "F-Measure = (2 * Precision * Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm0 = np.array([[1, 5, 34],[0, 56., 2], [0,0,120]])\n",
    "cm1 = np.array([[0., 0., 31.], [0., 44., 16.], [0., 7., 106.]])\n",
    "cm2 = np.array([[0,0,22], [0,71,0], [4,0,179]])\n",
    "cm3 = np.array([[0., 0., 37.], [1, 39,2], [0,0,128]])\n",
    "cm4 = np.array([[0., 0., 37.], [0,35,7], [0,1, 127]])\n",
    "    \n",
    "# sensitivities\n",
    "sens_reg = np.mean([ 0.025, 0, 0, 0,0])\n",
    "sens_pneu = np.mean([0.966, 0.733, 1, 0.929, 0.833])\n",
    "sens_covid = np.mean([1.0, 0.938, 0.978, 1, 0.992])\n",
    "# precisions\n",
    "prec_reg = np.mean([1.0, 0, 0, 0, 0])\n",
    "prec_pneu = np.mean([0.918, 0.863, 1, 1.0, 0.972])\n",
    "prec_covid = np.mean([0.769, 0.693, 0.891, 0.766, 0.743])\n",
    "\n",
    "accs_covidnet = [0.8119266, 0.73529, 0.905797, 0.80676, 0.78260]\n",
    "\n",
    "all_cms_cov_model = np.array([cm0, cm1, cm2, cm3, cm4])\n",
    "print(all_cms_cov_model.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_measure(prec, rec):\n",
    "    return (2*prec*rec)/(prec+rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output accuracy and balanced accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_cms_cov_net = np.sum(all_cms_cov_model, axis=0)\n",
    "bal_acc_covidnet = np.diag(added_cms_cov_net)/np.sum(added_cms_cov_net, axis=1)\n",
    "print(\"The accuracy and balanced accuracy of our model are:\")\n",
    "print(np.around(accs_covidnet,2),np.around(bal_acc_covidnet,2))\n",
    "print(\"MEAN ACC:\", round(np.mean(accs_covidnet), 2), \"MEAN BAL ACC:\", round(np.mean(bal_acc_covidnet),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make similar table for covid-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classes[\"Class\"] = df_classes.index\n",
    "df_classes.index = [\"our model\", \"our model\",\"our model\"]\n",
    "df_cov = df_classes.copy()\n",
    "df_cov.index = [\"covid-net\", \"covid-net\", \"covid-net\"]\n",
    "df_cov[\"Precision\"] = np.around([prec_covid, prec_pneu, prec_reg], 3).tolist()\n",
    "df_cov[\"Recall\"] = np.around([sens_covid, sens_pneu, sens_reg], 3).tolist()\n",
    "sens = np.array(compute_specificity(all_cms_cov_model))[[2,1,0]]\n",
    "df_cov[\"Specificity\"] = sens.tolist()\n",
    "df_cov[\"F1-score\"] = np.around([f_measure(p, r) for (p,r) in zip([prec_covid, prec_pneu, prec_reg], [sens_covid, sens_pneu, sens_reg])], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge both tables and output final table as latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_together = pd.concat([df_classes, df_cov])\n",
    "results_together[\"Sensitivity\"] = results_together[\"Recall\"]\n",
    "results_together = results_together[[\"Class\", \"Sensitivity\", \"Specificity\", \"Precision\", \"F1-score\", \"Frames\", \"Videos/Images\"]]\n",
    "print(results_together.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute video accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def video_accuracy(saved_logits, saved_gt, saved_files):\n",
    "    def preds_to_score(vid_preds_out):\n",
    "        return accuracy_score([p[2] for p in vid_preds_out], [p[1] for p in vid_preds_out])\n",
    "\n",
    "    def preds_to_balanced(vid_preds_out):\n",
    "        # print([p[1] for p in vid_preds_out], [p[2] for p in vid_preds_out])\n",
    "        return balanced_accuracy_score([p[2] for p in vid_preds_out], [p[1] for p in vid_preds_out])\n",
    "\n",
    "    scores_certainty, score_cert_bal = [], []\n",
    "    scores_majority, score_maj_bal = [], []\n",
    "    for i in range(len(saved_files)):\n",
    "        # print(\"-----------\", i, \"---------\")\n",
    "        filenames = np.array(saved_files[i])\n",
    "        only_videos = np.where(np.array([len(name.split(\".\"))==3 for name in filenames]))[0]\n",
    "        # print(len(only_videos), len(filenames))\n",
    "        logits_in = np.array(saved_logits[i])[only_videos]\n",
    "        files_in = filenames[only_videos]\n",
    "        gt_in = np.array(saved_gt[i])[only_videos]\n",
    "\n",
    "        vid_preds_certainty = average_certainty(logits_in, gt_in, files_in)\n",
    "        vid_preds_majority = majority_vote(np.argmax(logits_in, axis=1), gt_in, files_in)\n",
    "        scores_certainty.append(preds_to_score(vid_preds_certainty))\n",
    "        scores_majority.append(preds_to_score(vid_preds_majority))\n",
    "        score_maj_bal.append(preds_to_balanced(vid_preds_majority))\n",
    "        score_cert_bal.append(preds_to_balanced(vid_preds_certainty))\n",
    "    # print(\"certainty:\", scores_certainty)\n",
    "    # print(\"majority:\", scores_majority)\n",
    "    return scores_certainty, scores_majority, score_maj_bal, score_cert_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_certainty, scores_majority, score_maj_bal, score_cert_bal = video_accuracy(saved_logits, saved_gt, saved_files)\n",
    "scores_certainty, scores_majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_maj_bal, score_cert_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RESULTS VIDEO ACCURACY:\")\n",
    "print(\"Accuracies: \", scores_certainty, \"MEAN:\", round(np.mean(scores_certainty), 3))\n",
    "print(\"Balanced accs:\", score_cert_bal, \"MEAN:\", round(np.mean(score_cert_bal),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RESULTS VIDEO ACCURACY:\")\n",
    "print(\"Accuracies: \", scores_certainty, \"MEAN:\", round(np.mean(scores_certainty), 3))\n",
    "print(\"Balanced accs:\", score_cert_bal, \"MEAN:\", round(np.mean(score_cert_bal),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"number of images in each split\")\n",
    "for file_list in saved_files:\n",
    "    cutted_files = [files.split(\".\")[0] for files in file_list]\n",
    "    # print(np.unique(cutted_files))\n",
    "    image_number = [len(files.split(\".\"))!=3 for files in file_list]\n",
    "    print(np.sum(image_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../results_oct/base_new_3.dat\", \"rb\") as outfile:\n",
    "    (saved_logits, saved_gt, saved_files) = pickle.load(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum up confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cms = np.zeros((5,3,3))\n",
    "for s in range(5):\n",
    "    # print(saved_files[s])\n",
    "    gt_s = saved_gt[s]\n",
    "    pred_idx_s = np.argmax(np.array(saved_logits[s]), axis=1)\n",
    "    assert len(gt_s)==len(pred_idx_s)\n",
    "    cm = np.array(confusion_matrix(gt_s, pred_idx_s))\n",
    "    all_cms[s] = cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to make labels with std from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_label(data, text):\n",
    "    return (np.asarray([\"{0:.2f}\\n\".format(data)+u\"\\u00B1\"+\"{0:.2f}\".format(text) for data, text in zip(data.flatten(), text.flatten())])).reshape(3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (25,6))\n",
    "fig = plt.subplot(1,3,1)\n",
    "ax = fig.axes\n",
    "\n",
    "data_abs = np.sum(all_cms, axis=0)\n",
    "df_cm = pd.DataFrame(data_abs, index = [i for i in [\"COVID-19\", \"Pneumonia\", \"Healthy\"]],\n",
    "                  columns = [i for i in [\"COVID-19\", \"Pneumonia\", \"Healthy\"]])\n",
    "\n",
    "sn.set(font_scale=1.5)\n",
    "\n",
    "# plt.xticks(np.arange(3)+0.5,(\"COVID-19\", \"Pneumonia\", \"Normal\"), rotation=0, fontsize=\"17\", va=\"center\")\n",
    "plt.yticks(np.arange(3)+0.5,(\"COVID-19\", \"Pneumonia\", \"Healthy\"), rotation=0, fontsize=\"17\", va=\"center\")\n",
    "sn.heatmap(df_cm, annot=True, fmt=\"g\", cmap=\"YlGnBu\")\n",
    "ax.xaxis.tick_top()\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False) \n",
    "plt.xlabel('\\nPredictions', size=25)\n",
    "plt.ylabel('Ground truth', size=25)\n",
    "plt.title(\"Absolute values\\n\", size=30,fontweight=\"bold\")\n",
    "\n",
    "\n",
    "# PRECISION SUBPLOT\n",
    "fig = plt.subplot(1,3,2)\n",
    "ax = fig.axes\n",
    "\n",
    "\n",
    "data_prec = all_cms.copy()\n",
    "for i in range(5):\n",
    "    data_prec[i] = data_prec[i]/np.sum(data_prec[i], axis=0)\n",
    "prec_stds = np.std(data_prec, axis = 0)\n",
    "data_prec = np.mean(data_prec, axis=0)\n",
    "labels_prec = data_to_label(data_prec, prec_stds)\n",
    "\n",
    "df_cm = pd.DataFrame(data_prec, index = [i for i in [\"COVID-19\", \"Pneumonia\", \"Healthy\"]],\n",
    "                  columns = [i for i in [\"COVID-19\", \"Pneumonia\", \"Healthy\"]])\n",
    "sn.set(font_scale=1.5)\n",
    "ax.xaxis.tick_top()\n",
    "plt.ylabel(\"ground truth\")\n",
    "plt.xlabel(\"predictions\")\n",
    "plt.title(\"Precision\")\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False) \n",
    "plt.yticks(np.arange(3)+0.5,(\"COVID-19\", \"Pneumonia\", \"Healthy\"), rotation=0, fontsize=\"17\", va=\"center\")\n",
    "sn.heatmap(df_cm, annot=labels_prec, fmt='', cmap=\"YlGnBu\")\n",
    "plt.xlabel('\\nPredictions', size=25)\n",
    "plt.ylabel('Ground truth', size=25)\n",
    "plt.title(\"Precision\\n\", size=30,fontweight=\"bold\")\n",
    "\n",
    "plt.savefig(\"confusion_matrix_newdata.pdf\",bbox_inches='tight') #, bottom=0.2)\n",
    "\n",
    "\n",
    "#  SENSITIVITY SUBPLOT\n",
    "fig = plt.subplot(1,3,3)\n",
    "ax = fig.axes\n",
    "data_sens = all_cms.copy()\n",
    "for i in range(5):\n",
    "    sums_axis = np.sum(data_sens[i], axis=1)\n",
    "    data_sens[i] = np.array([data_sens[i,j,:]/sums_axis[j] for j in range(3)])\n",
    "sens_stds = np.std(data_sens, axis = 0)\n",
    "data_sens = np.mean(data_sens, axis=0)\n",
    "\n",
    "labels_sens = data_to_label(data_sens, sens_stds)\n",
    "df_cm = pd.DataFrame(data_sens, index = [i for i in [\"COVID-19\", \"Pneumonia\", \"Healthy\"]],\n",
    "                  columns = [i for i in [\"COVID-19\", \"Pneumonia\", \"Healthy\"]])\n",
    "# sn.set(font_scale=1.5)\n",
    "\n",
    "plt.yticks(np.arange(3)+0.5,(\"COVID-19\", \"Pneumonia\", \"Healthy\"), rotation=0, fontsize=\"17\", va=\"center\")\n",
    "#plt.xticks(np.arange(3)+0.5,(\"COVID-19\", \"Pneunomia\", \"Normal\"), rotation=0, fontsize=\"17\", va=\"center\")\n",
    "ax.xaxis.tick_top()\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False) \n",
    "\n",
    "sn.heatmap(df_cm, annot=labels_sens, fmt='', cmap=\"YlGnBu\")\n",
    "plt.xlabel('\\nPredictions', size=25)\n",
    "plt.ylabel('Ground truth', size=25)\n",
    "plt.title(\"Sensitivity (Recall)\\n\", size=30,fontweight=\"bold\")\n",
    "\n",
    "plt.savefig(\"../results_oct/confusion_matrix_all.pdf\",bbox_inches='tight') #, bottom=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute scores and curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_eval_points = np.linspace(0,1,200,endpoint=True)\n",
    "\n",
    "def roc_auc(saved_logits, saved_gt):\n",
    "    data, scores, roc_auc_std = [], [], []\n",
    "    max_points = []\n",
    "    for i in range(3):\n",
    "        out_roc = np.zeros((5, len(base_eval_points)))\n",
    "        out_prec = np.zeros((5, len(base_eval_points)))\n",
    "\n",
    "        roc_auc = []\n",
    "        max_acc = []\n",
    "        \n",
    "        # Iterate over folds\n",
    "        for k in range(5):\n",
    "            # get binary predictions for this class\n",
    "            gt =  (saved_gt[k] == i).astype(int)\n",
    "            # pred = saved_logits[k][:, i]\n",
    "            if np.any(saved_logits[k]<0):\n",
    "                pred = np.exp(np.array(saved_logits[k]))[:, i]\n",
    "            else:\n",
    "                pred = np.array(saved_logits[k])[:, i]\n",
    "            \n",
    "            roc_auc.append(roc_auc_score(gt, pred))\n",
    "            \n",
    "            precs, recs, fprs, julie_points = [], [], [], []\n",
    "            for j, thresh in enumerate(np.linspace(0,1.1,100, endpoint=True)):\n",
    "                preds_thresholded = (pred>thresh).astype(int)\n",
    "                tp = np.sum(preds_thresholded[gt==1])\n",
    "                p = np.sum(gt)\n",
    "                n = len(gt)-p\n",
    "                fp = np.sum(preds_thresholded[gt==0])\n",
    "                inverted = np.absolute(preds_thresholded - 1)\n",
    "                tn = np.sum(inverted[gt==0])\n",
    "                fn = np.sum(inverted[gt==1])\n",
    "                fpr = fp/float(n)\n",
    "                tpr = tp/float(p)\n",
    "                \n",
    "                if tp+fp ==0:\n",
    "                    precs.append(1)\n",
    "                else:\n",
    "                    precs.append(tp/(tp+fp))\n",
    "                recs.append(tpr)\n",
    "                fprs.append(fpr)\n",
    "                julie_points.append((tp+tn)/(tp+tn+fp+fn))\n",
    "\n",
    "            # clean\n",
    "            recs = np.asarray(recs)\n",
    "            precs = np.asarray(precs)\n",
    "            fprs = np.asarray(fprs)\n",
    "            sorted_inds = np.argsort(recs)\n",
    "            # prepare for precision-recall curve\n",
    "            precs_sorted = precs[sorted_inds]\n",
    "            recs_sorted = recs[sorted_inds]\n",
    "            precs_cleaned = precs_sorted[recs_sorted>0]\n",
    "            recs_cleaned = recs_sorted[recs_sorted>0]\n",
    "            precs_inter = np.interp(base_eval_points, recs_cleaned, precs_cleaned)\n",
    "            # prepare for roc-auc curve\n",
    "            sorted_inds = np.argsort(fprs)\n",
    "            recs_fpr_sorted = recs[sorted_inds]\n",
    "            fprs_sorted = fprs[sorted_inds]\n",
    "            roc_inter = np.interp(base_eval_points, fprs_sorted, recs_fpr_sorted)\n",
    "            # append current fold\n",
    "            out_prec[k] = precs_inter\n",
    "            out_roc[k] = roc_inter\n",
    "            \n",
    "            # compute recall of max acc:\n",
    "            max_acc.append(recs[np.argmax(julie_points)])\n",
    "\n",
    "        # out_curve = np.mean(np.asarray(out_curve), axis=0)\n",
    "\n",
    "        prec_mean = np.mean(out_prec, axis=0)\n",
    "        prec_std = np.std(out_prec, axis=0)\n",
    "        roc_mean = np.mean(out_roc, axis=0)\n",
    "        roc_std = np.std(out_roc, axis=0)\n",
    "        \n",
    "        # append scores\n",
    "        scores.append(round(np.mean(roc_auc),2))\n",
    "        roc_auc_std.append(round(np.std(roc_auc),2))\n",
    "        \n",
    "        # point of maximum accuracy\n",
    "        max_points.append(np.mean(max_acc))\n",
    "        \n",
    "        data.append((roc_mean, roc_std, prec_mean, prec_std))\n",
    "    return data, max_points, scores, roc_auc_std\n",
    "\n",
    "def closest(in_list, point):\n",
    "    return np.argmin(np.absolute(np.asarray(in_list)-point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc\n",
    "plt.rcParams['legend.title_fontsize'] = 20\n",
    "\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "# activate latex text rendering\n",
    "rc('text', usetex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../results_oct/base_new_3.dat\", \"rb\") as outfile:\n",
    "    (saved_logits, saved_gt, saved_files) = pickle.load(outfile)\n",
    "data, max_points, scores, roc_auc_std = roc_auc(saved_logits, saved_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"red\", \"orange\", \"green\"]\n",
    "classes = [\"COVID-19\", \"Pneumonia\", \"Healthy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC class comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.plot([0, 1], [0, 1], color='grey', lw=1.5, linestyle='--')\n",
    "for i in range(3):\n",
    "    roc_mean, roc_std, _, _ = data[i]\n",
    "    lab = classes[i]+\" (%.2f\"%scores[i]+\"$\\pm$\"+str(roc_auc_std[i])+\")\"\n",
    "    plt.plot(base_eval_points, roc_mean, 'k-', c=cols[i], label=lab, lw=3)\n",
    "    # print(len(r), max_points[i])\n",
    "    # print(base_eval_points[closest(roc_mean, max_points[i])], max_points[i])\n",
    "    plt.scatter(base_eval_points[closest(roc_mean, max_points[i])], max_points[i], s=150, marker=\"o\", c=cols[i])\n",
    "    plt.fill_between(base_eval_points, roc_mean-roc_std, roc_mean+roc_std, alpha=0.1, facecolor=cols[i])\n",
    "    plt.ylim(0,1.03)\n",
    "plt.xlim(-0.02,1)\n",
    "plt.ylabel(\"$\\\\bf{Sensitivity}$\", fontsize=20)\n",
    "plt.xlabel(\"$\\\\bf{False\\ positive\\ rate}$\", fontsize=20)\n",
    "plt.legend(fontsize=18, title=\"    $\\\\bf{Class}\\ \\\\bf(ROC-AUC)}$\") # \"\\n  $\\\\bf{(o:\\ maximal\\ accuracy)}$\")\n",
    "# plt.title(\"$\\\\bf{ROC\\ curves}$\", fontsize=15)\n",
    "plt.savefig(\"../results_oct/plots/roc_curves_cam.pdf\", bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.plot([1, 0], [0, 1], color='grey', lw=1.5, linestyle='--')\n",
    "for i in range(3):\n",
    "    _, _, prec_mean, prec_std = data[i]\n",
    "    # prec_cleaned = prec[rec>0]\n",
    "    # rec_cleaned = rec[rec>0]\n",
    "    # s2_cleaned = s2[rec>0]\n",
    "    lab = classes[i] # +\" (%.2f\"%scores[i]+\"$\\pm$\"+str(roc_auc_std[i])+\")\"\n",
    "    plt.plot(base_eval_points, prec_mean, 'k-', c=cols[i], label=lab, lw=3)\n",
    "    plt.fill_between(base_eval_points, prec_mean-prec_std, prec_mean+prec_std, alpha=0.1, facecolor=cols[i])\n",
    "plt.ylim(0,1.03)\n",
    "plt.xlim(-0.02,1)\n",
    "plt.ylabel(\"$\\\\bf{Precision}$\", fontsize=20)\n",
    "plt.xlabel(\"$\\\\bf{Recall}$\", fontsize=20)\n",
    "plt.legend(fontsize=18, title=\"    $\\\\bf{Class}$\") # \"\\n  $\\\\bf{(o:\\ maximal\\ accuracy)}$\")\n",
    "# plt.title(\"$\\\\bf{ROC\\ curves}$\", fontsize=15)\n",
    "plt.savefig(\"new_plots/prec_rec_curves_cam.pdf\", bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc\n",
    "plt.rcParams['legend.title_fontsize'] = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC-curve across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS = 1\n",
    "name_dict = {\"cross_val_gradcam_3\":\"VGG\",\"cross_val_cam_3\":\"VGG-CAM\", \"NAS_B_3\":\"NASNetMobile\",\"encoding_3\":\"Segment-Enc\", \"results_segment_3\":\"VGG-Segment\"}\n",
    "cols = [\"red\", \"orange\", \"green\", \"blue\", \"purple\"]\n",
    "classes = [\"COVID-19\", \"Pneumonia\", \"Healthy\"]\n",
    "# roc_auc_scores = np.mean(np.asarray(scores), axis=0)\n",
    "fig = plt.figure(figsize=(6,5))\n",
    "# plt.subplot(1,3,1)\n",
    "plt.plot([0, 1], [0, 1], color='grey', lw=1.5, linestyle='--')\n",
    "for i, model_data in enumerate([\"cross_val_gradcam_3.dat\", \"NAS_B_3.dat\", \"cross_val_cam_3.dat\", \"encoding_3.dat\", \"results_segment_3.dat\"]):\n",
    "    with open(model_data, \"rb\") as outfile:\n",
    "        (saved_logits, saved_gt, saved_files) = pickle.load(outfile)\n",
    "    data, max_points, scores, roc_auc_std = roc_auc(saved_logits, saved_gt)\n",
    "    roc_mean, roc_std, _, _ = data[CLASS]\n",
    "    lab = name_dict[model_data.split(\".\")[0]]+\" (%.2f\"%scores[CLASS]+\"$\\pm$\"+str(roc_auc_std[CLASS])+\")\"\n",
    "    \n",
    "    plt.plot(base_eval_points, roc_mean, 'k-', c=cols[i], label=lab, lw=3)\n",
    "    plt.scatter(base_eval_points[closest(roc_mean, max_points[CLASS])], max_points[CLASS], s=150, marker=\"o\", c=cols[i])\n",
    "    plt.fill_between(base_eval_points, roc_mean-roc_std, roc_mean+roc_std, alpha=0.1, facecolor=cols[i])\n",
    "    # plt.ylim(0,1.03)\n",
    "    # \n",
    "    # # roc auc plotting\n",
    "    # fp, prec, rec, s, s2 = data[CLASS]\n",
    "    # lab = name_dict[model_data.split(\".\")[0]]+\" (%.2f\"%scores[CLASS]+\"$\\pm$\"+str(roc_auc_std[CLASS])+\")\"\n",
    "    # plt.plot(fp, rec, 'k-', c=cols[i], label=lab, lw=3)\n",
    "    # # print(len(r), max_points[i])\n",
    "    # plt.scatter(fp[max_points[CLASS]], rec[max_points[CLASS]], s=150, marker=\"o\", c=cols[i])\n",
    "    # plt.fill_between(fp, rec-s, rec+s, alpha=0.1, facecolor=cols[i])\n",
    "plt.ylim(0,1.01)\n",
    "plt.xlim(-0.02,1)\n",
    "plt.ylabel(\"$\\\\bf{Sensitivity}$\", fontsize=15)\n",
    "plt.xlabel(\"$\\\\bf{False\\ positive\\ rate}$\", fontsize=15)\n",
    "plt.legend(fontsize=15, title=\"    $\\\\bf{Model}\\ \\\\bf(ROC-AUC)}$\") # \"\\n  $\\\\bf{(o:\\ maximal\\ accuracy)}$\")\n",
    "# plt.title(\"ROC-curve (COVID-19)\", fontsize=20)\n",
    "plt.savefig(\"new_plots/roc_curve\"+str(CLASS)+\".pdf\", bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-recall-curve across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS = 0\n",
    "fig = plt.figure(figsize=(6,5))\n",
    "\n",
    "for i, model_data in enumerate([\"cross_val_gradcam_3.dat\", \"NAS_B_3.dat\", \"cross_val_cam_3.dat\", \"encoding_3.dat\", \"results_segment_3.dat\"]):\n",
    "    with open(model_data, \"rb\") as outfile:\n",
    "        (saved_logits, saved_gt, saved_files) = pickle.load(outfile)\n",
    "    data, max_points, scores, roc_auc_std = roc_auc(saved_logits, saved_gt)\n",
    "    _, _, prec_mean, prec_std = data[CLASS]\n",
    "    lab = name_dict[model_data.split(\".\")[0]]\n",
    "    plt.plot(base_eval_points, prec_mean, 'k-', c=cols[i], label=lab, lw=3)\n",
    "    plt.fill_between(base_eval_points, prec_mean-prec_std, prec_mean+prec_std, alpha=0.1, facecolor=cols[i])\n",
    "    \n",
    "    # data, max_points, scores, roc_auc_std = roc_auc(saved_logits, saved_gt)\n",
    "    # # roc auc plotting\n",
    "    # fp, prec, rec, s, s2 = data[CLASS]\n",
    "    # prec_clean = np.asarray(prec)\n",
    "    # rec_clean = np.asarray(rec)\n",
    "    # prec_clean = prec_clean[rec_clean>0]\n",
    "    # s2_clean = np.asarray(s2)[rec_clean>0]\n",
    "    # rec_clean = rec_clean[rec_clean>0]\n",
    "    # lab = name_dict[model_data.split(\".\")[0]] # +\" (%.2f\"%scores[0]+\"$\\pm$\"+str(roc_auc_std[0])+\")\"\n",
    "    # plt.plot(rec_clean, prec_clean, 'k-', c=cols[i], label=lab, lw=3)\n",
    "    # # plt.plot(rec_cheat, prec_cheat, 'k-', c=cols[i], label=lab, lw=3)\n",
    "    # # print(len(r), max_points[i])\n",
    "    # # plt.scatter(prec[max_points[0]], rec[max_points[0]], s=150, marker=\"o\", c=cols[i])\n",
    "    # plt.fill_between(rec, prec-s2, prec+s2, alpha=0.1, facecolor=cols[i])\n",
    "\n",
    "plt.ylim(0,1.01)\n",
    "plt.xlim(-0.02,1.02)\n",
    "plt.ylabel(\"$\\\\bf{Precision}$\", fontsize=15)\n",
    "plt.xlabel(\"$\\\\bf{Recall}$\", fontsize=15)\n",
    "plt.legend(fontsize=15, title=\"    $\\\\bf{Model}}$\") # \"\\n  $\\\\bf{(o:\\ maximal\\ accuracy)}$\")\n",
    "# plt.title(\"Precision-Recall-curve (Healthy)\", fontsize=20)\n",
    "\n",
    "plt.savefig(\"new_plots/prec_rec_\"+str(CLASS)+\".pdf\", bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,5))\n",
    "ax = fig.axes\n",
    "\n",
    "# ABSOLUTE\n",
    "# data_confusion = np.sum(all_cms, axis=0)\n",
    "\n",
    "# PRECISION\n",
    "# data_confusion = all_cms.copy()\n",
    "# for i in range(5):\n",
    "#     data_confusion[i] = data_confusion[i]/np.sum(data_confusion[i], axis=0)\n",
    "# prec_stds = np.std(data_confusion, axis = 0)\n",
    "# data_confusion = np.mean(data_confusion, axis=0)\n",
    "# labels = data_to_label(data_confusion, prec_stds)\n",
    "\n",
    "# SENSITIVITY\n",
    "data_confusion = all_cms.copy()\n",
    "for i in range(5):\n",
    "    sums_axis = np.sum(data_confusion[i], axis=1)\n",
    "    data_confusion[i] = np.array([data_confusion[i,j,:]/sums_axis[j] for j in range(3)])\n",
    "sens_stds = np.std(data_confusion, axis = 0)\n",
    "data_confusion = np.mean(data_confusion, axis=0)\n",
    "\n",
    "labels = data_to_label(data_confusion, sens_stds)\n",
    "\n",
    "# ACTUAL PLOT\n",
    "\n",
    "df_cm = pd.DataFrame(data_confusion, index = [i for i in [\"COVID-19\", \"Pneumonia\", \"Healthy\"]],\n",
    "                  columns = [i for i in [\"COVID-19\", \"Pneumonia\", \"Healthy\"]])\n",
    "\n",
    "sn.set(font_scale=1.8)\n",
    "\n",
    "plt.xticks(np.arange(3)+0.5,(\"COVID-19\", \"Pneumonia\", \"Normal\"), fontsize=\"18\", va=\"center\")\n",
    "plt.yticks(np.arange(3)+0.5,(\"C\", \"P\", \"H\"), rotation=0, fontsize=\"18\", va=\"center\")\n",
    "# sn.heatmap(df_cm, annot=True, fmt=\"g\", cmap=\"YlGnBu\")\n",
    "sn.heatmap(df_cm, annot=labels, fmt='', cmap=\"YlGnBu\")\n",
    "# ax.xaxis.tick_bottom()\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=True)\n",
    "plt.xlabel(\"$\\\\bf{Predictions}$\", fontsize=20)\n",
    "plt.ylabel(\"$\\\\bf{Ground\\ truth}$\", fontsize=20)\n",
    "# plt.title(\"Confusion matrix (VGG2)\", fontsize=20) # \"Absolute values\\n\", size=30,fontweight=\"bold\")\n",
    "\n",
    "plt.savefig(\"../results_oct/conf_matrix_base_sens.pdf\", bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute roc-auc score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    roc_auc = []\n",
    "    for j in range(5):\n",
    "        # roc auc score\n",
    "        preds = saved_logits[j][:, i]\n",
    "        gt = (saved_gt[j] == i).astype(int)\n",
    "        # print(preds, gt)\n",
    "        roc_auc.append(roc_auc_score(gt, preds))\n",
    "    print(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Save predictions in csv (from logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"cross_val_gradcam_4.dat\", \"rb\") as outfile:\n",
    "    (saved_logits, saved_gt, saved_files) = pickle.load(outfile)\n",
    "    \n",
    "CLASSES = [\"covid\", \"pneumonia\", \"regular\", \"uninformative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for i in range(5):\n",
    "    df = pd.DataFrame()\n",
    "    df[\"fold\"] = [i for _ in range(len(saved_gt[i]))]\n",
    "    df[\"filename\"] = saved_files[i]\n",
    "    df[\"ground_truth\"] = saved_gt[i]\n",
    "    df[\"prediction\"] = np.argmax(saved_logits[i], axis=1)\n",
    "    df[\"probability\"] = np.max(saved_logits[i], axis=1)\n",
    "    dfs.append(df)\n",
    "    \n",
    "together = pd.concat(dfs)\n",
    "print(together.head())\n",
    "print(\"number of files\", len(together))\n",
    "print(\"Accuracy for all predictions\", np.sum(together[\"ground_truth\"].values == together[\"prediction\"].values)/len(together))\n",
    "relevant_classes = together[together[\"ground_truth\"]<3]\n",
    "print(len(relevant_classes))\n",
    "print(\"Accuracy for covid pneu relular predictions\", np.sum(relevant_classes[\"ground_truth\"].values == relevant_classes[\"prediction\"].values)/len(relevant_classes))\n",
    "\n",
    "# SAVE\n",
    "together.to_csv(\"predictions_vgg_4.csv\", index=False)\n",
    "relevant_classes.to_csv(\"predictions_vgg_3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save predictions in csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "path_to_csv = \"/Users/ninawiedemann/Desktop/Projects/covid19_pocus_ultrasound.nosync/pocovidnet/models/\"\n",
    "\n",
    "for filein in os.listdir(path_to_csv):\n",
    "    if filein[-3:]==\"csv\":\n",
    "        dfs.append(pd.read_csv(path_to_csv+filein))\n",
    "one_df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_name, frame_num, labels = [],[], []\n",
    "label_dict = {\"pne\":1, \"Pne\":1, \"Cov\":0, \"Reg\":2}\n",
    "for fn in one_df[\"Unnamed: 0\"]:\n",
    "    parts = fn.split(\".\")\n",
    "    vid_name.append(parts[0])\n",
    "    labels.append(label_dict[parts[0][:3]])\n",
    "    if len(parts)==2:\n",
    "        frame_num.append(None)\n",
    "    elif len(parts)==3:\n",
    "        frame_num.append(int(parts[1][9:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"covid (0)\", \"pneumonia (1)\", \"healthy (2)\"]\n",
    "trans_df = pd.DataFrame()\n",
    "trans_df[\"video\"] = vid_name\n",
    "trans_df[\"frame\"] = frame_num\n",
    "trans_df[\"label (0:cov, 1:pneu, 2:reg)\"] = labels # [classes[l] for l in labels]\n",
    "# add predictions\n",
    "preds = np.array(one_df[[\"0\",\"1\",\"2\"]])\n",
    "sorted_preds = np.argsort(preds, axis=1)\n",
    "trans_df[\"prediction (0:cov, 1:pneu, 2:reg)\"] = sorted_preds[:,2] # [classes[l] for l in sorted_preds[:,2]]\n",
    "trans_df[\"second_pred\"] = sorted_preds[:,1]\n",
    "trans_df[\"prob\"] = np.max(preds, axis=1)\n",
    "trans_df = trans_df.sort_values(by=[\"video\", \"frame\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = trans_df.groupby('video').agg({\"prob\":\"mean\", \"label (0:cov, 1:pneu, 2:reg)\":\"first\"})\n",
    "grouped[\"preds\"] = list(trans_df.groupby('video')[\"prediction (0:cov, 1:pneu, 2:reg)\"].apply(list))\n",
    "def most_frequent(List): \n",
    "    return max(set(List), key = List.count)\n",
    "grouped[\"majority_vote\"] = [most_frequent(val) for val in grouped[\"preds\"].values]\n",
    "gt_vid, preds_vid = (grouped[\"label (0:cov, 1:pneu, 2:reg)\"].values, grouped[\"majority_vote\"].values)\n",
    "gt, preds = (trans_df[\"label (0:cov, 1:pneu, 2:reg)\"].values, trans_df[\"prediction (0:cov, 1:pneu, 2:reg)\"].values)\n",
    "print(\"frame accuracy:\", np.sum(gt==preds)/len(gt), \"video accuracy\", np.sum(gt_vid==preds_vid)/len(gt_vid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.to_csv(\"predictions.csv\")\n",
    "trans_df.to_csv(\"framewise_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Covid-Net results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm0 = np.array([[24., 12., 12.], [ 0., 28.,  0.], [29.,  4., 30.]])\n",
    "cm1 = np.array([[  0.,   1.,  48.],[  0.,  22.,   0.],[  0.,   2., 109.]])\n",
    "cm2 = np.array([[17., 5., 13.],[ 2., 24., 0.],[ 0.,  0, 94.]])\n",
    "cm3 = np.array([[30., 0., 0.],[ 0., 25.,  0.],[ 3., 0, 85.]])\n",
    "cm4 = np.array([[19., 0., 8.],[ 6., 25., 0.], [ 0.,  0., 80.]])\n",
    "\n",
    "# sensitivities\n",
    "sens_reg = np.mean([0.5, 0, 0.486, 1.0, 0.704])\n",
    "sens_pneu = np.mean([1.0, 1.0, 0.923, 1.0, 0.806])\n",
    "sens_covid = np.mean([0.476, 0.982, 1.0, 0.966, 1.0])\n",
    "# precisions\n",
    "prec_reg = np.mean([0.453, 0, 0.895, 0.909, 0.76])\n",
    "prec_pneu = np.mean([0.636, 0.88, 0.828, 1.0, 1.0])\n",
    "prec_covid = np.mean([0.714, 0.694, 0.879, 1.0, 0.909])\n",
    "\n",
    "accs_covidnet = [0.58992805, 0.719, 0.871, 0.979, 0.89855]\n",
    "\n",
    "all_cms_cov_model = np.array([cm0, cm1, cm2, cm3, cm4])\n",
    "print(all_cms_cov_model.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to latex tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "50000000 / 9294192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_dir = \"../results_oct/\"\n",
    "class_map2 = {0:\"COVID-19\", 1:\"Pneumonia\", 2: \"Healthy\",3:\"Uninformative\"}\n",
    "for model in [\"base_new\", \"cam_new\", \"nasnet_new\", \"encoding\", \"segmented\"]: # , \"cam_4\", \"NAS_B_4\"]: # [\"vid_cam_3\", \"genesis_3\"]: # \n",
    "    mean_table = pd.read_csv(os.path.join(base_dir, model+\"_mean.csv\"))\n",
    "    std_table = pd.read_csv(os.path.join(base_dir, model+\"_std.csv\"))\n",
    "    print(\"----------\", model, \" ---------------\")\n",
    "    for i, row in mean_table.iterrows():\n",
    "        std_row = std_table.loc[i] # std_table[std_table[\"Unnamed: 0\"]==\"covid\"]\n",
    "        # if i==1:\n",
    "            # \"& $\", row[\"Accuracy\"],\"\\\\pm\",std_row[\"Accuracy\"],\"$ &\", \n",
    "        if i ==0:\n",
    "            print(round(row[\"Accuracy\"],2), std_row[\"Accuracy\"], row[\"Balanced\"], std_row[\"Balanced\"])\n",
    "        print(\"&\", class_map2[i],\n",
    "              \"& $\", round(row[\"Recall\"], 2), \"\\\\pm {\\scriptstyle\",std_row[\"Recall\"],\n",
    "              \"}$ & $\", round(row[\"Precision\"],2), \"\\\\pm {\\scriptstyle\",std_row[\"Precision\"],\n",
    "              \"}$ & $\", round(row[\"F1-score\"], 2), \"\\\\pm {\\scriptstyle\",std_row[\"F1-score\"], \n",
    "              \"}$ & $\", round(row[\"Specificity\"], 2), \"\\\\pm {\\scriptstyle\",std_row[\"Specificity\"],\n",
    "              \"}$ & $\",round(row[\"MCC\"], 2), \"\\\\pm {\\scriptstyle\",std_row[\"MCC\"], \"} $ \\\\\\\\\")\n",
    "        \n",
    "        # WO standard deviation\n",
    "        # print(\"& row[\"Accuracy\"],\"&\", class_map2[i],\"&\", row[\"Recall\"], \n",
    "         #         \"&\", row[\"Precision\"], \"&\", row[\"F1-score\"], \"&\", row[\"Specificity\"], \"&\", row[\"MCC\"], \"\\\\\\\\\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"model_comparison\"\n",
    "class_map2 = {0:\"COVID-19\", 1:\"Pneumonia\", 2: \"Healthy\"}\n",
    "for model in [\"frame_based_video_evaluation\", \"vid_based_video_evaluation\"]:\n",
    "    mean_table = pd.read_csv(os.path.join(base_dir, model+\".csv\"))\n",
    "    print(\"----------\", model)\n",
    "    for i, row in mean_table.iterrows():\n",
    "        std_row = std_table.loc[i] # std_table[std_table[\"Unnamed: 0\"]==\"covid\"]\n",
    "        # if i==1:\n",
    "            # \"& $\", row[\"Accuracy\"],\"\\\\pm\",std_row[\"Accuracy\"],\"$ &\", \n",
    "        print(row[\"Accuracy\"], row[\"Balanced\"])\n",
    "        \n",
    "        # WO standard deviation\n",
    "        print(\"&\", class_map2[i],\"&\", row[\"recall\"], \n",
    "                \"&\", row[\"precision\"], \"&\", row[\"f1-score\"], \"&\", row[\"Specificity\"], \"&\", row[\"MCC\"], \"\\\\\\\\\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
